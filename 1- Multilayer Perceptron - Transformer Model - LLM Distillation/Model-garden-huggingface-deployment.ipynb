{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32a24e04",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55bde2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Tuple\n",
    "from google.cloud import aiplatform\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "import vertexai\n",
    "from vertexai import model_garden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84c9677",
   "metadata": {},
   "source": [
    "### Load Environment Variables and Configuring gcloud Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed123395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "\n",
      "Credentials saved to file: [C:\\Users\\Luis\\AppData\\Roaming\\gcloud\\application_default_credentials.json]\n",
      "\n",
      "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
      "\n",
      "Quota project \"sacred-epigram-462314-r4\" was added to ADC which can be used by Google client libraries for billing and quota. Note that some services may still bill the project owning the resource.\n",
      "Operation \"operations/acat.p2-198380589718-12f700ee-52c3-48b1-a980-a021e9ffa0fc\" finished successfully.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "PROJECT_ID = os.environ[\"PROJECT_ID\"]\n",
    "REGION = os.environ[\"REGION\"]\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\", os.environ[\"HF_TOKEN\"])\n",
    "HUGGING_FACE_MODEL_ID = os.environ[\"HUGGING_FACE_MODEL_ID\"]\n",
    "MACHINE_TYPE = os.environ[\"MACHINE_TYPE\"]\n",
    "ACCELERATOR_TYPE = os.environ[\"ACCELERATOR_TYPE\"]\n",
    "ACCELERATOR_COUNT = int(os.environ.get(\"ACCELERATOR_COUNT\", 1))\n",
    "TGI_DOCKER_URI = os.environ[\"TGI_DOCKER_URI\"]\n",
    "SERVING_CONTAINER_IMAGE_URI = TGI_DOCKER_URI\n",
    "USE_DEDICATED_ENDPOINT = os.environ.get(\"USE_DEDICATED_ENDPOINT\", \"True\").lower() == \"true\"\n",
    "LABEL = os.environ.get(\"LABEL\", \"tgi\")\n",
    "\n",
    "! gcloud auth login\n",
    "! gcloud config set project {PROJECT_ID}\n",
    "! gcloud auth application-default set-quota-project {PROJECT_ID}\n",
    "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8f61b3",
   "metadata": {},
   "source": [
    "### Initializing Vertex AI Module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79e4bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "models, endpoints = {}, {}\n",
    "\n",
    "vertexai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d374fd0",
   "metadata": {},
   "source": [
    "### [Option 1] Deploy with Model Garden SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6582d584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying model: nvidia/Nemotron-Research-Reasoning-Qwen-1.5B\n",
      "LRO: projects/198380589718/locations/us-east1/operations/716479022615232512\n",
      "Start time: 2025-07-25 10:00:59.501086\n",
      "End time: 2025-07-25 10:24:14.924497\n",
      "Endpoint: projects/sacred-epigram-462314-r4/locations/us-east1/endpoints/mg-endpoint-1753452059\n"
     ]
    }
   ],
   "source": [
    "model = model_garden.OpenModel(HUGGING_FACE_MODEL_ID)\n",
    "endpoints[LABEL] = model.deploy(\n",
    "    machine_type=MACHINE_TYPE,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    accelerator_count= ACCELERATOR_COUNT,\n",
    "    hugging_face_access_token=HF_TOKEN,\n",
    "    use_dedicated_endpoint=USE_DEDICATED_ENDPOINT,\n",
    "    accept_eula=True\n",
    ")\n",
    "\n",
    "endpoint = endpoints[LABEL]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367fa313",
   "metadata": {},
   "source": [
    "### [Option 2] Deploy with customized configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f819b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "common_util = importlib.import_module(\n",
    "    \"vertex-ai-samples.community-content.vertex_model_garden.model_oss.notebook_util.common_util\"\n",
    ")\n",
    "\n",
    "def deploy_model_tgi(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    publisher: str,\n",
    "    publisher_model_id: str,\n",
    "    service_account: str = None,\n",
    "    machine_type: str = \"g2-standard-8\",\n",
    "    accelerator_type: str = \"NVIDIA_L4\",\n",
    "    accelerator_count: int = 1,\n",
    "    max_input_length: int = 2047,\n",
    "    max_total_tokens: int = 2048,\n",
    "    max_batch_prefill_tokens: int = 2048,\n",
    "    use_dedicated_endpoint: bool = False,\n",
    ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys models with TGI on GPU in Vertex AI.\"\"\"\n",
    "    endpoint = aiplatform.Endpoint.create(\n",
    "        display_name=f\"{model_name}-endpoint\",\n",
    "        dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
    "    )\n",
    "\n",
    "    env_vars = {\n",
    "        \"MODEL_ID\": model_id,\n",
    "        \"NUM_SHARD\": f\"{accelerator_count}\",\n",
    "        \"MAX_INPUT_LENGTH\": f\"{max_input_length}\",\n",
    "        \"MAX_TOTAL_TOKENS\": f\"{max_total_tokens}\",\n",
    "        \"MAX_BATCH_PREFILL_TOKENS\": f\"{max_batch_prefill_tokens}\",\n",
    "        \"DEPLOY_SOURCE\": \"notebook\",\n",
    "    }\n",
    "\n",
    "    # HF_TOKEN is not a compulsory field and may not be defined.\n",
    "    try:\n",
    "        if HF_TOKEN:\n",
    "            env_vars[\"HF_TOKEN\"] = HF_TOKEN\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    if service_account:\n",
    "        env_vars[\"SERVICE_ACCOUNT\"] = service_account\n",
    "\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=TGI_DOCKER_URI,\n",
    "        serving_container_ports=[8080],\n",
    "        serving_container_environment_variables=env_vars,\n",
    "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
    "        model_garden_source_model_name=(\n",
    "            f\"publishers/{publisher}/models/{publisher_model_id}\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "        system_labels={\n",
    "            \"NOTEBOOK_NAME\": \"model_garden_huggingface_tgi_deployment.ipynb\",\n",
    "            \"NOTEBOOK_ENVIRONMENT\": common_util.get_deploy_source(),\n",
    "        },\n",
    "    )\n",
    "    return model, endpoint\n",
    "\n",
    "\n",
    "models[\"tgi\"], endpoints[\"tgi\"] = deploy_model_tgi(\n",
    "    model_name=common_util.get_job_name_with_datetime(prefix=HUGGING_FACE_MODEL_ID),\n",
    "    model_id=HUGGING_FACE_MODEL_ID,\n",
    "    publisher=\"google\",\n",
    "    publisher_model_id=\"gemma2\",\n",
    "    service_account=\"\",\n",
    "    machine_type=MACHINE_TYPE,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    accelerator_count=ACCELERATOR_COUNT,\n",
    "    use_dedicated_endpoint=USE_DEDICATED_ENDPOINT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3af401",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60995922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, so I need to tell someone something interesting about myself being a predictive model. Hmm, where do I start? Well, first of all, I should think about what makes me unique in the predictive modeling field.\n",
      "\n",
      "Predictive models are like these mathematical or statistical equations that predict future outcomes based on past data. So maybe instead of just stating it's an AI or some software tool, perhaps talk about my methodology?\n",
      "\n",
      "Wait, but the user asked for something interesting about myself as a predictive model. Maybe I can say how versatile I am or how impactful my work has been?\n",
      "\n",
      "But wait another thought: Perhaps focus more on my role and achievements beyond just being a model. Like, have I contributed significantly to certain fields? Or did I win awards for having good predictions?\n",
      "\n",
      "Alternatively, maybe mention something about personal growth through working on predictive models. But since they specifically mentioned \"predicative model,\" which seems off.\n",
      "\n",
      "Or perhaps explain how prediction helps solve real-world problems. That might be more applicable.\n",
      "\n",
      "Let me try again. The question is asking somethingInteresting about myself regarding predicting things. Maybe highlight something about innovation or creativity in developing such models. Or touch upon how analytics has become part of daily life because of this technology.\n",
      "\n",
      "Another angle could be how advanced algorithms allow personalized experiencesâ€”like recommendation systems or customer segmentation.\n",
      "\n",
      "Since I'm an assistant thinking out loud here, let me structure this:\n",
      "\n",
      "Maybe start with passion or curiosity behind building predictive models. Then add some detail about their applications or impact.\n",
      "\n",
      "For example: \"I was born into the world of data analysis and machine learning, using predictive models to uncover patterns hidden in datasets.\"\n",
      "\n",
      "Or maybe emphasize the ability to understand complex relationships within data without prior knowledge required.\n",
      "\n",
      "Or discuss the challenge versus success aspectâ€”how sometimes models fail but contribute valuable insights when used correctly.\n",
      "\n",
      "Oh wait! Another idea: How significant it is that these models not only make predictions but also drive decisions by providing actionable insights.\n",
      "\n",
      "So putting together one sentence: Predictive models... aren't just toolsâ€”they're transformational because they help mine own conclusions from vast amounts of data to inform strategic choices.\n",
      "\n",
      "That sounds concise and highlights innovation.\n",
      "</think>\n",
      "\n",
      "My journey begins as an enthusiast exploring the intriguing intersection of data science and mathematics. As a persistent learner, I've developed predictive models capable of identifying patterns and forecasting trends across various domains. These models haven't merely serve as mere tools; they've driven transformative decision-making processes by translating complex data into actionable advice. Now, every insight comes at the stroke of an algorithm designed to uncover meaningful connections within datasets.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Tell me something interesting about yourself as a predictive model\"\n",
    "\n",
    "# Construct the request payload in the chat completions format\n",
    "instances = [\n",
    "    {\n",
    "        \"@requestFormat\": \"chatCompletions\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": 0.8,\n",
    "        \"max_new_tokens\": -1,\n",
    "        \"top_k\": 50,\n",
    "        \"top_p\": 1.0,\n",
    "        \"repetition_penalty\": 1.2\n",
    "    }\n",
    "]\n",
    "\n",
    "response = endpoints[LABEL].predict(\n",
    "    instances=instances,\n",
    "    use_dedicated_endpoint=USE_DEDICATED_ENDPOINT\n",
    ")\n",
    "\n",
    "text_content = response.predictions['choices'][0]['message']['content']\n",
    "print(text_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d0b9c9",
   "metadata": {},
   "source": [
    "### Delete the models and endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc38aa11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undeploying Endpoint model: projects/198380589718/locations/us-east1/endpoints/mg-endpoint-1753452059\n",
      "Undeploy Endpoint model backing LRO: projects/198380589718/locations/us-east1/endpoints/mg-endpoint-1753452059/operations/5280877244955230208\n",
      "Endpoint model undeployed. Resource name: projects/198380589718/locations/us-east1/endpoints/mg-endpoint-1753452059\n",
      "Deleting Endpoint : projects/198380589718/locations/us-east1/endpoints/mg-endpoint-1753452059\n",
      "Endpoint deleted. . Resource name: projects/198380589718/locations/us-east1/endpoints/mg-endpoint-1753452059\n",
      "Deleting Endpoint resource: projects/198380589718/locations/us-east1/endpoints/mg-endpoint-1753452059\n",
      "Delete Endpoint backing LRO: projects/198380589718/locations/us-east1/operations/2975034235741536256\n",
      "Endpoint resource projects/198380589718/locations/us-east1/endpoints/mg-endpoint-1753452059 deleted.\n"
     ]
    }
   ],
   "source": [
    "for endpoint in endpoints.values():\n",
    "    endpoint.delete(force=True)\n",
    "\n",
    "# Delete models.\n",
    "for model in models.values():\n",
    "    model.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
